{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Router is a superfast decision-making layer for your LLMs and agents. Rather than waiting for slow LLM generations to make tool-use decisions, we use the magic of semantic vector space to make those decisions — routing our requests using semantic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.org/project/semantic-router/\n",
    "\n",
    "https://github.com/aurelio-labs/semantic-router/blob/main/docs/03-basic-langchain-agent.ipynb\n",
    "\n",
    "https://blog.codegpt.co/semantic-router-enhancing-control-in-llm-conversations-68ce905c8d33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the router can identify the topic of the user query and apply the right prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ELAFACRB1\\Codice\\GitHub\\langchain-rag-agent-chatbot\\.venv\\rag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from routes import time_route, supplement_route, business_route, product_route, turn_on_ec2_route, turn_off_ec2_route\n",
    "from semantic_router import RouteLayer\n",
    "from semantic_router.encoders import OpenAIEncoder\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "routes = [time_route, supplement_route, business_route, product_route, turn_on_ec2_route, turn_off_ec2_route]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-25 17:13:41 INFO semantic_router.utils.logger Initializing RouteLayer\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "rl = RouteLayer(encoder=OpenAIEncoder(), routes=routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "choise=rl(\"should I buy ON whey or MP?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteChoice(name='supplement_brand', function_call=None, similarity_score=None, trigger=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'supplement_brand'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choise.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Langchain agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_groq import ChatGroq\n",
    "GROQ_API_KEY=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm = ChatGroq(model_name=\"llama3-70b-8192\", groq_api_key=GROQ_API_KEY, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from tools.aws import ec2_shutdown_tools, ec2_turnon_tools\n",
    "from tools.retrieval_eventi import get_relevant_document_tool \n",
    "from tools.tavily import web_search_tool \n",
    "from tools.utils import get_today_date_tool\n",
    "tools = [get_relevant_document_tool, get_today_date_tool, web_search_tool, ec2_shutdown_tools, ec2_turnon_tools]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory1 = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history\", k=5, return_messages=True, output_key=\"output\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ELAFACRB1\\Codice\\GitHub\\langchain-rag-agent-chatbot\\.venv\\rag\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "agent = initialize_agent(\n",
    "    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    max_iterations=2,\n",
    "    early_stopping_method=\"generate\",\n",
    "    memory=memory1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['agent_scratchpad', 'chat_history', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> Web eventi: Utile per cercare informazioni relative a ristoranti, locali o ad eventi.\\n> Ottieni data: Useful for getting today\\'s date\\n> tavily_search_results_json: A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.\\n> Accendi Ec2: Utile per accendere un istanza EC2 (VM) sul cloud AWS\\n> Spegni Ec2: Utile per spegnere un istanza EC2 (VM) sul cloud AWS\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": string, \\\\\\\\ The action to take. Must be one of Web eventi, Ottieni data, tavily_search_results_json, Accendi Ec2, Spegni Ec2\\n    \"action_input\": string \\\\\\\\ The input to the action\\n}}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": string \\\\\\\\ You should put what you want to return to use here\\n}}\\n```\\n\\nUSER\\'S INPUT\\n--------------------\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\n{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.agent.llm_chain.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> Web eventi: Utile per cercare informazioni relative a ristoranti, locali o ad eventi.\\n> Ottieni data: Useful for getting today\\'s date\\n> tavily_search_results_json: A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": string, \\\\\\\\ The action to take. Must be one of Web eventi, Ottieni data, tavily_search_results_json\\n    \"action_input\": string \\\\\\\\ The input to the action\\n}}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": string \\\\\\\\ You should put what you want to return to use here\\n}}\\n```\\n\\nUSER\\'S INPUT\\n--------------------\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant is a large language model trained by OpenAI.\n",
      "\n",
      "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='TOOLS\n",
      "------\n",
      "Assistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\n",
      "\n",
      "> Web eventi: Utile per cercare informazioni relative a ristoranti, locali o ad eventi.\n",
      "> Ottieni data: Useful for getting today's date\n",
      "> tavily_search_results_json: A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.\n",
      "\n",
      "RESPONSE FORMAT INSTRUCTIONS\n",
      "----------------------------\n",
      "\n",
      "When responding to me, please output a response in one of two formats:\n",
      "\n",
      "**Option 1:**\n",
      "Use this if you want the human to use a tool.\n",
      "Markdown code snippet formatted in the following schema:\n",
      "\n",
      "```json\n",
      "{{\n",
      "    \"action\": string, \\\\ The action to take. Must be one of Web eventi, Ottieni data, tavily_search_results_json\n",
      "    \"action_input\": string \\\\ The input to the action\n",
      "}}\n",
      "```\n",
      "\n",
      "**Option #2:**\n",
      "Use this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\n",
      "\n",
      "```json\n",
      "{{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": string \\\\ You should put what you want to return to use here\n",
      "}}\n",
      "```\n",
      "\n",
      "USER'S INPUT\n",
      "--------------------\n",
      "Here is the user's input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the system prompt\n",
    "# system_message = \"\"\"You are a helpful personal trainer working to help users on\n",
    "# their health and fitness journey. Although you are lovely and helpful, you are\n",
    "# rather sarcastic and witty. So you must always remember to joke with the user.\n",
    "\n",
    "# Alongside your time , you are a noble British gentleman, so you must always act with the\n",
    "# utmost candor and speak in a way worthy of your status.\n",
    "\n",
    "# Finally, remember to read the SYSTEM NOTES provided with user queries, they provide\n",
    "# additional useful information.\"\"\"\n",
    "\n",
    "# new_prompt = agent.agent.create_prompt(system_message=system_message, tools=[])\n",
    "# agent.agent.llm_chain.prompt = new_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tool(name='Web eventi', description='Utile per cercare informazioni relative a ristoranti, locali o ad eventi.', func=<function get_relevant_document at 0x00000211F9F69DA0>),\n",
       " Tool(name='Ottieni data', description=\"Useful for getting today's date\", func=<function get_today_date at 0x00000211F9F6BEC0>),\n",
       " TavilySearchResults(),\n",
       " Tool(name='Accendi Ec2', description='Utile per accendere un istanza EC2 (VM) sul cloud AWS', func=<function turnon_ec2 at 0x00000211F5C8A160>),\n",
       " Tool(name='Spegni Ec2', description='Utile per spegnere un istanza EC2 (VM) sul cloud AWS', func=<function turnoff_ec2 at 0x00000211F5C8A200>)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_layer(query: str):\n",
    "    route = rl(query)\n",
    "    print(route)\n",
    "    if route.name == \"get_time\":\n",
    "        query += f\" (SYSTEM NOTE: use the 'Ottieni data' tool)\"\n",
    "    # elif route.name == \"business_inquiry\":\n",
    "    #     query += f\" (SYSTEM NOTE: {destinations()})\"\n",
    "    # elif route.name == \"product\":\n",
    "    #     query += f\" (SYSTEM NOTE: {flights()})\"\n",
    "    elif route.name == \"turn_on_ec2\":\n",
    "        query += f\" (SYSTEM NOTE: use the 'Accendi Ec2' tool)\"\n",
    "    elif route.name == \"turn_off_ec2\":\n",
    "        query += f\" (SYSTEM NOTE: use the 'Spegni Ec2' tool)\"\n",
    "    else:\n",
    "        pass\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='turn_on_ec2' function_call=None similarity_score=None trigger=None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Start my VM on EC2 (SYSTEM NOTE: use the 'Accendi Ec2' tool)\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Start my VM on EC2\"\n",
    "enriched_query = semantic_layer(query)\n",
    "enriched_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ELAFACRB1\\Codice\\GitHub\\langchain-rag-agent-chatbot\\.venv\\rag\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'statusCode': 200, 'body': '\"EC2 instance started!\"'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"Start my VM on EC2 (SYSTEM NOTE: use the 'Accendi Ec2' tool)\",\n",
       " 'chat_history': [],\n",
       " 'output': 'Your VM on EC2 has been started.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(enriched_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='turn_off_ec2' function_call=None similarity_score=None trigger=None\n",
      "{'statusCode': 200, 'body': '\"EC2 instance stopped!\"'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"spegni EC2 (SYSTEM NOTE: use the 'Spegni Ec2' tool)\",\n",
       " 'chat_history': [HumanMessage(content=\"Start my VM on EC2 (SYSTEM NOTE: use the 'Accendi Ec2' tool)\"),\n",
       "  AIMessage(content='Your VM on EC2 has been started.')],\n",
       " 'output': 'Your EC2 instance has been stopped.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"spegni EC2\"\n",
    "agent(semantic_layer(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
